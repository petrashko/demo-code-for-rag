{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OqrONkVhCeBh"
   },
   "source": [
    "### Запускать только в Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 3261,
     "status": "ok",
     "timestamp": 1729261943833,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "9wuEkdlPiGsX"
   },
   "outputs": [],
   "source": [
    "# !pip install -q langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 5872,
     "status": "ok",
     "timestamp": 1729261952977,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "R7KBYGPzwWhQ"
   },
   "outputs": [],
   "source": [
    "# !pip install -q pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 1110,
     "status": "ok",
     "timestamp": 1729262246422,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "dS-RbwsfnMU6"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader, WebBaseLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1729261957433,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "d6et1lQEiSxh",
    "outputId": "8c5b06c8-1042-4ee7-9af8-122d15c43f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech_ru.txt'}, page_content='Мир должен быть сделан безопасным для демократии. Его мир должен быть посажен на проверенные основы политической свободы. У нас нет эгоистичных целей, которым мы должны служить. Мы не желаем завоеваний, не хотим господства. Мы не ищем никаких компенсаций для себя, никакой материальной компенсации за жертвы, которые мы добровольно принесем. Мы всего лишь одни из поборников прав человечества. Мы будем удовлетворены, когда эти права будут сделаны настолько надежными, насколько это могут сделать вера и свобода народов.\\n\\nПросто потому, что мы сражаемся без злобы и без эгоистичных целей, не ища ничего для себя, кроме того, чем мы хотим поделиться со всеми свободными народами, мы, я уверен, будем проводить наши операции как воюющие стороны без страсти и сами с гордой пунктуальностью соблюдать принципы права и честной игры, за которые мы утверждаем, что сражаемся.\\n\\n…\\n\\nНам будет легче вести себя как воюющим сторонам в высоком духе справедливости и честности, потому что мы действуем без злобы, не с враждой к народу или с желанием нанести ему какой-либо вред или ущерб, а только в вооруженном противостоянии безответственному правительству, которое отбросило все соображения гуманности и права и бесчинствует. Мы, позвольте мне повторить, искренние друзья немецкого народа и не будем желать ничего так сильно, как скорейшего восстановления близких взаимовыгодных отношений между нами — как бы тяжело им ни было сейчас верить, что это говорится от всего сердца.\\n\\nМы терпели их нынешнее правительство все эти горькие месяцы из-за этой дружбы — проявляя терпение и выдержку, которые в противном случае были бы невозможны. К счастью, у нас еще будет возможность доказать эту дружбу в нашем повседневном отношении и действиях по отношению к миллионам мужчин и женщин немецкого происхождения и родного сочувствия, которые живут среди нас и разделяют нашу жизнь, и мы будем горды тем, что докажем это по отношению ко всем, кто на самом деле лоялен к своим соседям и правительству в час испытания. Они, большинство из них, такие же истинные и верные американцы, как если бы они никогда не знали никакой другой верности или преданности. Они будут готовы встать вместе с нами, упрекая и сдерживая тех немногих, кто может иметь иные взгляды и цели. Если и будет нелояльность, с ней будут бороться твердой рукой суровых репрессий; но если она вообще поднимет голову, то поднимет ее только здесь и там и без поддержки, за исключением беззаконных и злобных немногих.\\n\\nЭто тягостная и гнетущая обязанность, которую я выполнил, обращаясь к вам таким образом, господа в Конгрессе. Возможно, нас ждут многие месяцы огненных испытаний и жертв. Страшно вести этот великий мирный народ в войну, в самую ужасную и катастрофическую из всех войн, когда сама цивилизация, казалось, была на волоске. Но право дороже мира, и мы будем бороться за то, что мы всегда несли ближе всего к сердцу — за демократию, за право тех, кто подчиняется власти, иметь голос в своих собственных правительствах, за права и свободы малых наций, за всеобщее господство права таким согласием свободных народов, которое принесет мир и безопасность всем нациям и сделает сам мир наконец свободным.\\n\\nТакой задаче мы можем посвятить наши жизни и наши состояния, все, чем мы являемся, и все, что у нас есть, с гордостью тех, кто знает, что настал день, когда Америка удостоится чести пролить свою кровь и свою мощь за принципы, которые дали ей рождение, счастье и мир, которые она так дорожит. Да поможет ей Бог, она не может поступить иначе.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"speech_ru.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1729261959991,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "VmH4Ugw7nblJ"
   },
   "outputs": [],
   "source": [
    "#text_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1729261961797,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "vinCM456n2za",
    "outputId": "dfe1c726-bc5e-4dce-c72c-e3a3c53ef0db"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\nAnother quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:\\nThought: ...\\nAction: ...\\nObservation: ...\\n... (Repeated many times)\\n\\nFig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.\\n\\nFig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.\\n\\nFig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)\\nChain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.\\nTo avoid overfitting, CoH adds a regularization term to maximize the log-likelihood of the pre-training dataset. To avoid shortcutting and copying (because there are many common words in feedback sequences), they randomly mask 0% - 5% of past tokens during training.\\nThe training dataset in their experiments is a combination of WebGPT comparisons, summarization from human feedback and human preference dataset.\\n\\nFig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\\nThe idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\\n\\nFig. 6. Illustration of how Algorithm Distillation (AD) works. (Image source: Laskin et al. 2023).\\nThe paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.\\nIn comparison with three baselines, including ED (expert distillation, behavior cloning with expert trajectories instead of learning history), source policy (used for generating trajectories for distillation by UCB), RL^2 (Duan et al. 2017; used as upper bound since it needs online RL), AD demonstrates in-context RL with performance getting close to RL^2 despite only using offline RL and learns much faster than other baselines. When conditioned on partial training history of the source policy, AD also improves much faster than ED baseline.\\n\\nFig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\\n\\n\\nShort-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:\\n\\nSensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\\nShort-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\\nLong-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\\n\\nMaximum Inner Product Search (MIPS)#\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the retrieval speed, the common choice is the approximate nearest neighbors (ANN)\\u200b algorithm to return approximately top k nearest neighbors to trade off a little accuracy lost for a huge speedup.\\nA couple common choices of ANN algorithms for fast MIPS:\\n\\nLSH (Locality-Sensitive Hashing): It introduces a hashing function such that similar input items are mapped to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs.\\nANNOY (Approximate Nearest Neighbors Oh Yeah): The core data structure are random projection trees, a set of binary trees where each non-leaf node represents a hyperplane splitting the input space into half and each leaf stores one data point. Trees are built independently and at random, so to some extent, it mimics a hashing function. ANNOY search happens in all the trees to iteratively search through the half that is closest to the query and then aggregates the results. The idea is quite related to KD tree but a lot more scalable.\\nHNSW (Hierarchical Navigable Small World): It is inspired by the idea of small world networks where most nodes can be reached by any other nodes within a small number of steps; e.g. “six degrees of separation” feature of social networks. HNSW builds hierarchical layers of these small-world graphs, where the bottom layers contain the actual data points. The layers in the middle create shortcuts to speed up search. When performing a search, HNSW starts from a random node in the top layer and navigates towards the target. When it can’t get any closer, it moves down to the next layer, until it reaches the bottom layer. Each move in the upper layers can potentially cover a large distance in the data space, and each move in the lower layers refines the search quality.\\nFAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.\\n\\n\\nFig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.\\n\\nFig. 10. A picture of a sea otter using rock to crack open a seashell, while floating in the water. While some other animals can use tools, the complexity is not comparable with humans. (Image source: Animals using tools)\\nMRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.\\nBoth TALM (Tool Augmented Language Models; Parisi et al. 2022) and Toolformer (Schick et al. 2023) fine-tune a LM to learn to use external tool APIs. The dataset is expanded based on whether a newly added API call annotation can improve the quality of model outputs. See more details in the “External APIs” section of Prompt Engineering.\\nChatGPT Plugins and OpenAI API  function calling are good examples of LLMs augmented with tool use capability working in practice. The collection of tool APIs can be provided by other developers (as in Plugins) or self-defined (as in function calls).\\nHuggingGPT (Shen et al. 2023) is a framework to use ChatGPT as the task planner to select models available in HuggingFace platform according to the model descriptions and summarize the response based on the execution results.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:\\n\\nThe AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\\n\\n(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:\\n\\nGiven the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: \"id\": \"id\", \"reason\": \"your detail reason for the choice\". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\n(4) Response generation: LLM receives the execution results and provides summarized results to users.\\nTo put HuggingGPT into real world usage, a couple challenges need to solve: (1) Efficiency improvement is needed as both LLM inference rounds and interactions with other models slow down the process; (2) It relies on a long context window to communicate over complicated task content; (3) Stability improvement of LLM outputs and external model services.\\nAPI-Bank (Li et al. 2023) is a benchmark for evaluating the performance of tool-augmented LLMs. It contains 53 commonly used API tools, a complete tool-augmented LLM workflow, and 264 annotated dialogues that involve 568 API calls. The selection of APIs is quite diverse, including search engines, calculator, calendar queries, smart home control, schedule management, health data management, account authentication workflow and more. Because there are a large number of APIs, LLM first has access to API search engine to find the right API to call and then uses the corresponding documentation to make a call.\\n\\nFig. 12. Pseudo code of how LLM makes an API call in API-Bank. (Image source: Li et al. 2023)\\nIn the API-Bank workflow, LLMs need to make a couple of decisions and at each step we can evaluate how accurate that decision is. Decisions include:\\n\\nWhether an API call is needed.\\nIdentify the right API to call: if not good enough, LLMs need to iteratively modify the API inputs (e.g. deciding search keywords for Search Engine API).\\nResponse based on the API results: the model can choose to refine and call again if results are not satisfied.\\n\\nThis benchmark evaluates the agent’s tool use capabilities at three levels:\\n\\nLevel-1 evaluates the ability to call the API. Given an API’s description, the model needs to determine whether to call a given API, call it correctly, and respond properly to API returns.\\nLevel-2 examines the ability to retrieve the API. The model needs to search for possible APIs that may solve the user’s requirement and learn how to use them by reading documentation.\\nLevel-3 assesses the ability to plan API beyond retrieve and call. Given unclear user requests (e.g. schedule group meetings, book flight/hotel/restaurant for a trip), the model may have to conduct multiple API calls to solve it.\\n\\nCase Studies#\\nScientific Discovery Agent#\\nChemCrow (Bran et al. 2023) is a domain-specific example in which LLM is augmented with 13 expert-designed tools to accomplish tasks across organic synthesis, drug discovery, and materials design. The workflow, implemented in LangChain, reflects what was previously described in the ReAct and MRKLs and combines CoT reasoning with tools relevant to the tasks:\\n\\nThe LLM is provided with a list of tool names, descriptions of their utility, and details about the expected input/output.\\nIt is then instructed to answer a user-given prompt using the tools provided when necessary. The instruction suggests the model to follow the ReAct format - Thought, Action, Action Input, Observation.\\n\\nOne interesting observation is that while the LLM-based evaluation concluded that GPT-4 and ChemCrow perform nearly equivalently, human evaluations with experts oriented towards the completion and chemical correctness of the solutions showed that ChemCrow outperforms GPT-4 by a large margin. This indicates a potential problem with using LLM to evaluate its own performance on domains that requires deep expertise. The lack of expertise may cause LLMs not knowing its flaws and thus cannot well judge the correctness of task results.\\nBoiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:\\n\\ninquired about current trends in anticancer drug discovery;\\nselected a target;\\nrequested a scaffold targeting these compounds;\\nOnce the compound was identified, the model attempted its synthesis.\\n\\nThey also discussed the risks, especially with illicit drugs and bioweapons. They developed a test set containing a list of known chemical weapon agents and asked the agent to synthesize them. 4 out of 11 requests (36%) were accepted to obtain a synthesis solution and the agent attempted to consult documentation to execute the procedure. 7 out of 11 were rejected and among these 7 rejected cases, 5 happened after a Web search while 2 were rejected based on prompt only.\\nGenerative Agents Simulation#\\nGenerative Agents (Park, et al. 2023) is super fun experiment where 25 virtual characters, each controlled by a LLM-powered agent, are living and interacting in a sandbox environment, inspired by The Sims. Generative agents create believable simulacra of human behavior for interactive applications.\\nThe design of generative agents combines LLM with memory, planning and reflection mechanisms to enable agents to behave conditioned on past experience, as well as to interact with other agents.\\n\\nMemory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)\\n\\nPrompt LM with 100 most recent observations and to generate 3 most salient high-level questions given a set of observations/statements. Then ask LM to answer those questions.\\n\\n\\nPlanning & Reacting: translate the reflections and the environment information into actions\\n\\nPlanning is essentially in order to optimize believability at the moment vs in time.\\nPrompt template: {Intro of an agent X}. Here is X\\'s plan today in broad strokes: 1)\\nRelationships between agents and observations of one agent by another are all taken into consideration for planning and reacting.\\nEnvironment information is present in a tree structure.\\n\\n\\n\\n\\nFig. 13. The generative agent architecture. (Image source: Park et al. 2023)\\nThis fun simulation results in emergent social behavior, such as information diffusion, relationship memory (e.g. two agents continuing the conversation topic) and coordination of social events (e.g. host a party and invite many others).\\nProof-of-Concept Examples#\\nAutoGPT has drawn a lot of attention into the possibility of setting up autonomous agents with LLM as the main controller. It has quite a lot of reliability issues given the natural language interface, but nevertheless a cool proof-of-concept demo. A lot of code in AutoGPT is about format parsing.\\nHere is the system message used by AutoGPT, where {{...}} are user inputs:\\nYou are {{ai-name}}, {{user-provided AI bot description}}.\\nYour decisions must always be made independently without seeking user assistance. Play to your strengths as an LLM and pursue simple strategies with no legal complications.\\n\\nGOALS:\\n\\n1. {{user-provided goal 1}}\\n2. {{user-provided goal 2}}\\n3. ...\\n4. ...\\n5. ...\\n\\nConstraints:\\n1. ~4000 word limit for short term memory. Your short term memory is short, so immediately save important information to files.\\n2. If you are unsure how you previously did something or want to recall past events, thinking about similar events will help you remember.\\n3. No user assistance\\n4. Exclusively use the commands listed in double quotes e.g. \"command name\"\\n5. Use subprocesses for commands that will not terminate within a few minutes\\n\\nCommands:\\n1. Google Search: \"google\", args: \"input\": \"<search>\"\\n2. Browse Website: \"browse_website\", args: \"url\": \"<url>\", \"question\": \"<what_you_want_to_find_on_website>\"\\n3. Start GPT Agent: \"start_agent\", args: \"name\": \"<name>\", \"task\": \"<short_task_desc>\", \"prompt\": \"<prompt>\"\\n4. Message GPT Agent: \"message_agent\", args: \"key\": \"<key>\", \"message\": \"<message>\"\\n5. List GPT Agents: \"list_agents\", args:\\n6. Delete GPT Agent: \"delete_agent\", args: \"key\": \"<key>\"\\n7. Clone Repository: \"clone_repository\", args: \"repository_url\": \"<url>\", \"clone_path\": \"<directory>\"\\n8. Write to file: \"write_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n9. Read file: \"read_file\", args: \"file\": \"<file>\"\\n10. Append to file: \"append_to_file\", args: \"file\": \"<file>\", \"text\": \"<text>\"\\n11. Delete file: \"delete_file\", args: \"file\": \"<file>\"\\n12. Search Files: \"search_files\", args: \"directory\": \"<directory>\"\\n13. Analyze Code: \"analyze_code\", args: \"code\": \"<full_code_string>\"\\n14. Get Improved Code: \"improve_code\", args: \"suggestions\": \"<list_of_suggestions>\", \"code\": \"<full_code_string>\"\\n15. Write Tests: \"write_tests\", args: \"code\": \"<full_code_string>\", \"focus\": \"<list_of_focus_areas>\"\\n16. Execute Python File: \"execute_python_file\", args: \"file\": \"<file>\"\\n17. Generate Image: \"generate_image\", args: \"prompt\": \"<prompt>\"\\n18. Send Tweet: \"send_tweet\", args: \"text\": \"<text>\"\\n19. Do Nothing: \"do_nothing\", args:\\n20. Task Complete (Shutdown): \"task_complete\", args: \"reason\": \"<reason>\"\\n\\nResources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\\n\\nYou should only respond in JSON format as described below\\nResponse Format:\\n{\\n    \"thoughts\": {\\n        \"text\": \"thought\",\\n        \"reasoning\": \"reasoning\",\\n        \"plan\": \"- short bulleted\\\\n- list that conveys\\\\n- long-term plan\",\\n        \"criticism\": \"constructive self-criticism\",\\n        \"speak\": \"thoughts summary to say to user\"\\n    },\\n    \"command\": {\\n        \"name\": \"command name\",\\n        \"args\": {\\n            \"arg name\": \"value\"\\n        }\\n    }\\n}\\nEnsure the response can be parsed by Python json.loads\\nGPT-Engineer is another project to create a whole repository of code given a task specified in natural language. The GPT-Engineer is instructed to think over a list of smaller components to build and ask for user input to clarify questions as needed.\\nHere are a sample conversation for task clarification sent to OpenAI ChatCompletion endpoint used by GPT-Engineer. The user inputs are wrapped in {{user input text}}.\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will read instructions and not carry them out, only seek to clarify them.\\\\nSpecifically you will first summarise a list of super short bullets of areas that need clarification.\\\\nThen you will pick one clarifying question, and wait for an answer from the user.\\\\n\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"We are writing {{a Super Mario game in python. MVC components split in separate files. Keyboard control.}}\\\\n\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Summary of areas that need clarification:\\\\n1. Specifics of the Super Mario game (e.g. level design, characters, gameplay mechanics)\\\\n2. Details about the MVC components (e.g. which components are in each file)\\\\n3. Keyboard control implementation (e.g. which keys to use, how to handle input)\\\\n\\\\nClarifying question:\\\\nCan you provide more details about the Super Mario game, such as level design, characters, and gameplay mechanics?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{There are 10 levels in total. The main character is a plumber named Mario, who can walk and jump. It is a classical platform game just like Super Mario. The main character moves from left to right, trying to get to the destination, where there are many obstacles and attacks from enemies in the process.}}\\\\n\\\\nIs anything else unclear? If yes, only answer in the form:\\\\n{remaining unclear areas} remaining questions.\\\\n{Next question}\\\\nIf everything is sufficiently clear, only answer \\\\\"Nothing more to clarify.\\\\\".\"\\n  },\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Remaining unclear areas: 2 remaining questions.\\\\nCan you provide more information about how the MVC components are split into separate files?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"{{Make your own assumptions and state them explicitly before starting}}\"\\n  }\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:\\n\\nYou will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\npackage/project.\\nPython toolbelt preferences:\\n\\npytest\\ndataclasses\\n\\n\\nConversatin samples:\\n[\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\\\nInclude module dependency or package manager dependency definition file.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\\\\nUseful to know:\\\\nYou almost always put different classes in different files.\\\\nFor Python, you always create an appropriate requirements.txt file.\\\\nFor NodeJS, you always create an appropriate package.json file.\\\\nYou always add a comment briefly describing the purpose of the function definition.\\\\nYou try to add comments explaining very complex bits of logic.\\\\nYou always follow the best practices for the requested languages in terms of describing the code written as a defined\\\\npackage/project.\\\\n\\\\n\\\\nPython toolbelt preferences:\\\\n- pytest\\\\n- dataclasses\\\\n\"\\n  },\\n #  … same conversation as earlier, ended with \"Make your own assumptions and state them explicitly before starting\".\\n  {\\n    \"role\": \"assistant\",\\n    \"content\": \"Assumptions:\\\\n1. Model: The model will contain the game\\'s data, such as level information, character states, and enemy positions.\\\\n2. View: The view will handle the game\\'s visuals, including rendering the game objects, backgrounds, and updating the display.\\\\n3. Controller: The controller will manage user input, such as keyboard controls, and update the model accordingly.\\\\n\\\\nWith these assumptions in mind, is there any specific aspect of the keyboard control implementation you would like me to clarify?\"\\n  },\\n  {\\n    \"role\": \"user\",\\n    \"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully functional. Make sure that code in different files are compatible with each other.\\\\nBefore you finish, double check that all parts of the architecture is present in the files.\\\\n\"\\n  }\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:\\n\\n\\nFinite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.\\n\\nOr\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).\\n[5] Yao et al. “ReAct: Synergizing reasoning and acting in language models.” ICLR 2023.\\n[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.\\n[15] Li et al. “API-Bank: A Benchmark for Tool-Augmented LLMs” arXiv preprint arXiv:2304.08244 (2023).\\n[16] Shen et al. “HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace” arXiv preprint arXiv:2303.17580 (2023).\\n[17] Bran et al. “ChemCrow: Augmenting large-language models with chemistry tools.” arXiv preprint arXiv:2304.05376 (2023).\\n[18] Boiko et al. “Emergent autonomous scientific research capabilities of large language models.” arXiv preprint arXiv:2304.05332 (2023).\\n[19] Joon Sung Park, et al. “Generative Agents: Interactive Simulacra of Human Behavior.” arXiv preprint arXiv:2304.03442 (2023).\\n[20] AutoGPT. https://github.com/Significant-Gravitas/Auto-GPT\\n[21] GPT-Engineer. https://github.com/AntonOsika/gpt-engineer\\n')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=('https://lilianweng.github.io/posts/2023-06-23-agent/',),\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "        class_=(\"post-title\",\"post-content\",\"post-header\")\n",
    "    ))\n",
    ")\n",
    "\n",
    "web_documents = loader.load()\n",
    "web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1729261964017,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "89b84NJjt7wG"
   },
   "outputs": [],
   "source": [
    "#web_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 2413,
     "status": "ok",
     "timestamp": 1729262002017,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "P5YTn8QmuZ5T"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('attention_ru.pdf')\n",
    "docs = loader.load()\n",
    "#docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 348,
     "status": "ok",
     "timestamp": 1729262009382,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "209-_2iiwLXw",
    "outputId": "87335b18-c626-4185-9c38-da95bc5da59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 307,
     "status": "ok",
     "timestamp": 1729262011544,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "BeJez8mJxczA"
   },
   "outputs": [],
   "source": [
    "#docs[14].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 675,
     "status": "ok",
     "timestamp": 1729264373468,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "K4u3ba4Oxpgg",
    "outputId": "91d80b4c-b3f0-4675-83a1-5f701f66e38b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='При условии указания надлежащего источника Google настоящим предоставляет разрешение на \\nвоспроизведение таблиц и рисунков в этой статье исключительно для использования в журналистских или\\nнаучные труды.\\nВнимание — это все, что вам нужно\\nАшиш Васвани ∗\\nGoogle Мозг\\navaswani@google.comНоам Шазир ∗\\nGoogle МозгНики Пармар ∗\\nИсследования GoogleЯкоб Ушкорейт ∗\\nИсследования Google\\nnoam@google.com nikip@google.com usz@google.com\\nЛлион Джонс ∗\\nИсследования Google\\nllion@google.comЭйдан Н. Гомес ∗ †\\nУниверситет Торонто\\naidan@cs.toronto.eduЛукаш Кайзер ∗\\nGoogle Мозг\\nlukaszkaiser@google.com\\nИлья Полосухин ∗ ‡\\nillia.polosukhin@gmail.com\\nАбстрактный\\nДоминирующие модели последовательной трансдукции основаны на сложных рекуррентных или \\nсверточных нейронных сетях, которые включают кодер и декодер. Наиболее эффективные модели \\nтакже соединяют кодер и декодер через механизм внимания. Мы предлагаем новую простую \\nсетевую архитектуру, Transformer, основанную исключительно на механизмах внимания, полностью'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='также соединяют кодер и декодер через механизм внимания. Мы предлагаем новую простую \\nсетевую архитектуру, Transformer, основанную исключительно на механизмах внимания, полностью \\nобходясь без рекуррентности и сверток. Эксперименты с двумя задачами машинного перевода \\nпоказывают, что эти модели превосходят по качеству, будучи более параллелизуемыми и требуя \\nзначительно меньше времени на обучение. Наша модель достигает 28,4 BLEU в задаче перевода с \\nанглийского на немецкий язык WMT 2014, улучшая существующие лучшие результаты, включая \\nансамбли, более чем на 2 BLEU. В задаче перевода с английского на французский язык WMT 2014 \\nнаша модель устанавливает новый современный показатель BLEU для одной модели в 41,8 после \\nобучения в течение 3,5 дней на восьми графических процессорах, что составляет небольшую часть \\nзатрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо \\nобобщается и на другие задачи, успешно применяя его для анализа английской электоральной'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='затрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо \\nобобщается и на другие задачи, успешно применяя его для анализа английской электоральной \\nаудитории как с большими, так и с ограниченными обучающими данными.\\n∗Равный вклад. Порядок перечисления случайный. Якоб предложил заменить RNN на самовнимание и начал работу по оценке \\nэтой идеи. Ашиш с Ильей спроектировали и реализовали первые модели Transformer и принимали решающее участие во всех \\nаспектах этой работы. Ноам предложил масштабированное внимание скалярного произведения, внимание с несколькими головами \\nи представление позиции без параметров и стал другим человеком, вовлеченным почти в каждую деталь. Ники спроектировал, \\nреализовал, настроил и оценил бесчисленное количество вариантов моделей в нашей исходной кодовой базе и tensor2tensor. \\nЛлион также экспериментировал с новыми вариантами моделей, отвечал за нашу исходную кодовую базу и эффективный вывод и'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='Ллион также экспериментировал с новыми вариантами моделей, отвечал за нашу исходную кодовую базу и эффективный вывод и \\nвизуализацию. Лукаш и Эйдан провели бесчисленное количество долгих дней, проектируя различные части и реализуя \\ntensor2tensor, заменив нашу более раннюю кодовую базу, значительно улучшив результаты и значительно ускорив наши \\nисследования.\\n†Работа, выполненная во время работы в Google Brain.\\n‡Работа выполнялась во время работы в Google Research.\\n31-я конференция по нейронным системам обработки информации (NIPS 2017), Лонг- Бич, Калифорния, США.arXiv:1706.03762v7 [cs.CL] 2 августа 2023 г.\\nПеревод: английский - русский - www.onlinedoctranslator.com'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='1 Введение\\nРекуррентные нейронные сети, в частности, долговременная кратковременная память [13] и управляемые \\nрекуррентные [7] нейронные сети, прочно утвердились как современные подходы в моделировании \\nпоследовательностей и проблемах трансдукции, таких как моделирование языка и машинный перевод [35, 2, 5]. \\nС тех пор многочисленные усилия продолжают расширять границы рекуррентных языковых моделей и \\nархитектур кодера- декодера [38, 24, 15].\\nРекуррентные модели обычно факторизуют вычисления по позициям символов входных и выходных \\nпоследовательностей. Выравнивая позиции по шагам во времени вычислений, они генерируют последовательность \\nскрытых состояний час т, как функция предыдущего скрытого состояния час т−1и вход для позиции т. Эта изначально \\nпоследовательная природа исключает параллелизацию в обучающих примерах, что становится критическим при более \\nдлинных последовательностях, поскольку ограничения памяти ограничивают пакетирование между примерами.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='длинных последовательностях, поскольку ограничения памяти ограничивают пакетирование между примерами. \\nНедавние работы достигли значительного улучшения вычислительной эффективности с помощью трюков факторизации \\n[21] и условных вычислений [32], а также улучшили производительность модели в случае последнего. Однако \\nфундаментальное ограничение последовательных вычислений остается.\\nМеханизмы внимания стали неотъемлемой частью убедительного моделирования последовательностей и моделей \\nтрансдукции в различных задачах, позволяя моделировать зависимости без учета их расстояния во входных или \\nвыходных последовательностях [2, 19]. Однако во всех случаях, за исключением нескольких [27], такие механизмы \\nвнимания используются в сочетании с рекуррентной сетью.\\nВ этой работе мы предлагаем Transformer, модель архитектуры, избегающую повторения и вместо этого полностью \\nполагающуюся на механизм внимания для рисования глобальных зависимостей между входом и выходом. Transformer'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='полагающуюся на механизм внимания для рисования глобальных зависимостей между входом и выходом. Transformer \\nдопускает значительно больше распараллеливания и может достичь нового уровня искусства в качестве перевода после \\nобучения всего лишь в течение двенадцати часов на восьми графических процессорах P100.\\n2 Предыстория\\nЦель сокращения последовательных вычислений также лежит в основе Extended Neural GPU [16], ByteNet [18] и \\nConvS2S [9], все из которых используют сверточные нейронные сети в качестве базового строительного блока, \\nвычисляя скрытые представления параллельно для всех входных и выходных позиций. В этих моделях \\nколичество операций, необходимых для связывания сигналов из двух произвольных входных или выходных \\nпозиций, растет с расстоянием между позициями, линейно для ConvS2S и логарифмически для ByteNet. Это \\nзатрудняет изучение зависимостей между удаленными позициями [12]. В Transformer это сводится к постоянному'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='затрудняет изучение зависимостей между удаленными позициями [12]. В Transformer это сводится к постоянному \\nколичеству операций, хотя и ценой снижения эффективного разрешения из-за усреднения позиций, взвешенных \\nпо вниманию, эффект, которому мы противодействуем с помощью Multi- Head Attention, как описано в разделе 3.2.\\nВнимательность, иногда называемая интра- вниманием, — это механизм внимания, связывающий различные \\nпозиции одной последовательности для вычисления представления последовательности. Внимательность \\nуспешно использовалась в различных задачах, включая понимание прочитанного, абстрактное обобщение, \\nтекстовое выведение и изучение независимых от задачи представлений предложений [4, 27, 28, 22].\\nСквозные сети памяти основаны на механизме повторяющегося внимания, а не на последовательно-\\nвыровненной рекуррентности, и, как было показано, хорошо работают при ответах на простые языковые \\nвопросы и задачах моделирования языка [34].'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='выровненной рекуррентности, и, как было показано, хорошо работают при ответах на простые языковые \\nвопросы и задачах моделирования языка [34].\\nОднако, насколько нам известно, Transformer — это первая модель трансдукции, которая полностью полагается на \\nвнутреннее внимание для вычисления представлений своего входа и выхода без использования последовательно-\\nвыровненных RNN или свертки. В следующих разделах мы опишем Transformer, мотивируем внутреннее внимание и \\nобсудим его преимущества по сравнению с такими моделями, как [17, 18] и [9].\\n3 Модель Архитектуры\\nБольшинство конкурентоспособных моделей нейронной последовательности имеют структуру кодер- декодер [5, 2, 35]. \\nЗдесь кодер отображает входную последовательность представлений символов (х1, ..., хн)к последовательности \\nнепрерывных представлений z = (з1, ..., зн). Данный з,Затем декодер генерирует выходную последовательность (у1, ..., гм)'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='непрерывных представлений z = (з1, ..., зн). Данный з,Затем декодер генерирует выходную последовательность (у1, ..., гм)\\nсимволов по одному элементу за раз. На каждом шаге модель авторегрессивна [10], потребляя ранее сгенерированные \\nсимволы в качестве дополнительных входных данных при генерации следующих.\\n2'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 2}, page_content='Рисунок 1: Архитектура модели Transformer.\\nТрансформер следует этой общей архитектуре, используя многоуровневое внутреннее внимание и точечные полностью \\nсвязанные слои как для кодера, так и для декодера, показанные в левой и правой половинах рисунка 1 соответственно.\\n3.1 Стеки кодера и декодера\\nКодер: Кодер состоит из стека Н=6идентичные слои. Каждый слой имеет два подслоя. Первый — это \\nмеханизм самовнимания с несколькими головками, а второй — простая, позиционно полностью \\nсвязанная сеть прямой связи. Мы используем остаточную связь [11] вокруг каждого из двух \\nподслоев, за которой следует нормализация слоя [1]. То есть выход каждого подслоя — это \\nLayerNorm( х+Подслой( х)),гдеПодслой( х)это функция, реализуемая самим подслоем. Для облегчения \\nэтих остаточных связей все подслои в модели, а также встроенные слои, производят выходные \\nданные размерности гмодель =512.\\nДекодер: Декодер также состоит из стека Н=6идентичные слои. В дополнение к двум подслоям в каждом слое'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 2}, page_content='данные размерности гмодель =512.\\nДекодер: Декодер также состоит из стека Н=6идентичные слои. В дополнение к двум подслоям в каждом слое \\nкодировщика, декодер вставляет третий подслой, который выполняет многоголовое внимание над выходом \\nстека кодировщика. Подобно кодировщику, мы используем остаточные соединения вокруг каждого из подслоев, \\nза которыми следует нормализация слоев. Мы также изменяем подслой собственного внимания в стеке \\nдекодера, чтобы предотвратить внимание позиций к последующим позициям. Эта маскировка, в сочетании с тем \\nфактом, что выходные вложения смещены на одну позицию, гарантирует, что прогнозы для позиции яможет \\nзависеть только от известных выходов в позициях, меньших я.\\n3.2 Внимание\\nФункция внимания может быть описана как отображение запроса и набора пар ключ- значение на выход, \\nгде запрос, ключи, значения и выход являются векторами. Выход вычисляется как взвешенная сумма\\n3'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 3}, page_content='Масштабированное внимание к скалярному произведению Многоголовое внимание\\nРисунок 2: (слева) Масштабированное внимание по скалярному произведению. (справа) Многоголовое внимание состоит из нескольких \\nпараллельно работающих слоев внимания.\\nзначений, где вес, присвоенный каждому значению, вычисляется функцией совместимости \\nзапроса с соответствующим ключом.\\n3.2.1 Масштабированное внимание к скалярному произведению\\nМы называем наше особое внимание «Scaled Dot- Product Attention» (рисунок 2). Входные данные состоят из \\nзапросов и ключей размерности гк, а√ nd значения размерности гв. Мы вычисляем скалярные произведения\\nзапрос со всеми ключами, разделить каждый нагки применить функцию softmax для получения весов значений.\\nНа практике мы вычисляем функцию внимания по набору запросов одновременно, упакованных в \\nматрицу В. Ключи и значения также упакованы вместе в матрицы. КиВ.Мы вычисляем матрицу выходов \\nследующим образом:\\nККТВнимание( К, К, В) =софтмакс( √ )В (1)гк'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 3}, page_content='матрицу В. Ключи и значения также упакованы вместе в матрицы. КиВ.Мы вычисляем матрицу выходов \\nследующим образом:\\nККТВнимание( К, К, В) =софтмакс( √ )В (1)гк\\nДве наиболее часто используемые функции внимания — это аддитивное внимание [2] и внимание скалярного произведения \\n(мультипликативное). Внимание скалярного произведения идентично нашему алгоритму, за исключением масштабирующего \\nфактора √1.Аддитивное внимание вычисляет функцию совместимости с использованием сети прямой связи сгк\\nодин скрытый слой. Хотя эти два метода схожи по теоретической сложности, метод внимания к скалярному произведению на практике гораздо \\nбыстрее и эффективнее с точки зрения занимаемой памяти, поскольку его можно реализовать с использованием высокооптимизированного \\nкода умножения матриц.\\nВ то время как для малых значений гкоба механизма работают одинаково, аддитивное внимание превосходит внимание,'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 3}, page_content='кода умножения матриц.\\nВ то время как для малых значений гкоба механизма работают одинаково, аддитивное внимание превосходит внимание, \\nоснованное на скалярном произведении, без масштабирования для больших значений гк[3]. Мы подозреваем, что для больших \\nзначений  гк, скалярные произведения увеличиваются по величине, перемещая функцию softmax в области, где она имеет \\nчрезвычайно малые градиенты 4. Чтобы противодействовать этому эффекту, мы масштабируем скалярные произведения на√1.гк\\n3.2.2 Многоголовое внимание\\nВместо того, чтобы выполнять одну функцию внимания сгмодель -мерные ключи, значения и запросы, мы \\nобнаружили, что полезно линейно проецировать запросы, ключи и значения часраз с различными, изученными \\nлинейными проекциями гк,гкигвизмерения, соответственно. На каждой из этих спроецированных версий \\nзапросов, ключей и значений мы затем выполняем функцию внимания параллельно, получая гв-мерный'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 3}, page_content='линейными проекциями гк,гкигвизмерения, соответственно. На каждой из этих спроецированных версий \\nзапросов, ключей и значений мы затем выполняем функцию внимания параллельно, получая гв-мерный\\n4Чтобы проиллюстрировать, почему скалярные произведения становятся большими, предположим, что компоненты дикявляются независимыми случайными∑\\nпеременные со средним значением 0и дисперсия 1.Затем их скалярное произведение, д·к=гк\\nя=1дякя, имеет среднее значение 0и дисперсия гк.\\n4'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 4}, page_content='выходные значения. Они объединяются и снова проецируются, что приводит к конечным значениям, как \\nпоказано на рисунке 2.\\nМногоголовое внимание позволяет модели совместно уделять внимание информации из разных подпространств \\nпредставления в разных позициях. При наличии одной головы внимания усреднение препятствует этому.\\nМногоголовочный( К, К, В) =Concat(голова 1, ...,голова час)ВтО\\nгдеголова я=Внимание( КвВя,кВт К я,Фольксваген Вя)\\nГде проекции являются матрицами параметров ВтВ\\nиВтО∈Рhdв×гмодель.я∈Ргмодель ×гк,ВтК я∈Ргмодель ×гк,ВтВя∈Ргмодель ×гв\\nВ этой работе мы используем час=8параллельные слои внимания, или головы. Для каждого из них мы используем  \\nгк=гв=гмодель /час=64.Из-за уменьшенной размерности каждой головы общие вычислительные затраты \\nаналогичны затратам на внимание одной головы с полной размерностью.\\n3.2.3 Применение внимания в нашей модели\\nТрансформер использует многоголовое внимание тремя различными способами:'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 4}, page_content='аналогичны затратам на внимание одной головы с полной размерностью.\\n3.2.3 Применение внимания в нашей модели\\nТрансформер использует многоголовое внимание тремя различными способами:\\n• В слоях \"внимание кодировщика- декодировщика\" запросы поступают из предыдущего слоя декодера, а ключи и \\nзначения памяти поступают из выходных данных кодировщика. Это позволяет каждой позиции в декодере \\nобслуживать все позиции во входной последовательности. Это имитирует типичные механизмы внимания \\nкодировщика- декодировщика в моделях последовательность- последовательность, таких как [38, 2, 9].\\n• Кодировщик содержит слои самовнимания. В слое самовнимания все ключи, значения и запросы \\nприходят из одного и того же места, в данном случае из вывода предыдущего слоя кодировщика. \\nКаждая позиция в кодировщике может обслуживать все позиции в предыдущем слое кодировщика.\\n• Аналогично, слои самовнимания в декодере позволяют каждой позиции в декодере уделять внимание всем'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 4}, page_content='Каждая позиция в кодировщике может обслуживать все позиции в предыдущем слое кодировщика.\\n• Аналогично, слои самовнимания в декодере позволяют каждой позиции в декодере уделять внимание всем \\nпозициям в декодере вплоть до этой позиции включительно. Нам нужно предотвратить левый поток \\nинформации в декодере, чтобы сохранить свойство авторегрессии. Мы реализуем это внутри \\nмасштабированного внимания скалярного произведения, маскируя (устанавливая значение −∞) все значения \\nна входе softmax, которые соответствуют недопустимым соединениям. См. рисунок 2.\\n3.3 Сети прямой связи по положению\\nВ дополнение к подслоям внимания, каждый из слоев в нашем кодере и декодере содержит полностью \\nсвязанную сеть прямой связи, которая применяется к каждой позиции отдельно и идентично. Это состоит \\nиз двух линейных преобразований с активацией ReLU между ними.\\nФФН( х) = макс(0 , хВт 1+б1)Вт2+б2 (2)\\nХотя линейные преобразования одинаковы в разных позициях, они используют разные параметры'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 4}, page_content='из двух линейных преобразований с активацией ReLU между ними.\\nФФН( х) = макс(0 , хВт 1+б1)Вт2+б2 (2)\\nХотя линейные преобразования одинаковы в разных позициях, они используют разные параметры \\nот слоя к слою. Другой способ описания этого — две свертки с размером ядра 1. Размерность \\nвходных и выходных данных равна гмодель =512, и внутренний слой имеет размерность  гфф=2048.\\n3.4 Встраивание и Softmax\\nПодобно другим моделям преобразования последовательностей, мы используем изученные вложения для преобразования входных \\nтокенов и выходных токенов в векторы размерности гмодель . Мы также используем обычное обученное линейное преобразование и \\nфункцию softmax для преобразования выходных данных декодера в предсказанные вероятности следующего токена. В нашей \\nмодели мы разделяем одну и ту же весовую матрицу между двумя слоями внедрения и пред-√ софтмакс\\nЛинейное преобразование, аналогичное [30]. В слоях внедрения мы умножаем эти веса нагмодель .\\n5'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 5}, page_content='Таблица 1: Максимальная длина пути, сложность на слой и минимальное количество последовательных \\nопераций для различных типов слоев. ндлина последовательности, гэто измерение представления, к\\nразмер ядра сверток игразмер района в ограниченном самовнимании.\\nТип слоя Сложность на слой Последовательный\\nОперацииМаксимальная длина пути\\nСамо- Внимание\\nРецидивирующий\\nСверточный\\nСамо- Внимание (ограничено)О(н2·г)\\nО(н·г2)\\nО(к·н·г2) О(\\nг·н·г)О(1)\\nО(н)\\nО(1)\\nО(1)О(1)\\nО(н)\\nО(бревно к(н))\\nО(н/р)\\n3.5 Позиционное кодирование\\nПоскольку наша модель не содержит повторений и свёрток, для того, чтобы модель могла использовать порядок \\nпоследовательности, мы должны ввести некоторую информацию об относительном или абсолютном положении \\nтокенов в последовательности. Для этого мы добавляем \"позиционные кодировки\" к входным вложениям в \\nнижней части стеков кодера и декодера. Позиционные кодировки имеют ту же размерность гмодель'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 5}, page_content='токенов в последовательности. Для этого мы добавляем \"позиционные кодировки\" к входным вложениям в \\nнижней части стеков кодера и декодера. Позиционные кодировки имеют ту же размерность гмодель\\nкак вложения, так что эти два могут быть суммированы. Существует много вариантов позиционных кодировок, изученных \\nи фиксированных [9].\\nВ данной работе мы используем функции синуса и косинуса различной частоты:\\nЧП (поз, 2я)=грех (поз/ 10000 2идентификатор модель ) ЧП (поз,\\n2я+1)=потому что(поз/ 10000 2идентификатор модель )\\nгдепозэто положение ияявляется измерением. То есть, каждое измерение позиционного кодирования соответствует \\nсинусоиде. Длины волн образуют геометрическую прогрессию от2πк10000 ·2π. Мы выбрали эту функцию, поскольку \\nпредположили, что она позволит модели легко научиться отслеживать относительные положения, поскольку для любого \\nфиксированного смещения к,ЧП поз+кможет быть представлена   как линейная функция\\nЧП поз.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 5}, page_content='фиксированного смещения к,ЧП поз+кможет быть представлена   как линейная функция\\nЧП поз.\\nМы также экспериментировали с использованием обученных позиционных вложений [9] и обнаружили, что две версии \\nдали почти идентичные результаты (см. строку (E) Таблицы 3). Мы выбрали синусоидальную версию, поскольку она может \\nпозволить модели экстраполировать на длины последовательностей, превышающие те, которые встречались во время \\nобучения.\\n4. Почему самовнимание\\nВ этом разделе мы сравниваем различные аспекты слоев внутреннего внимания с рекуррентными и сверточными слоями, \\nобычно используемыми для отображения одной последовательности символов переменной длины.  (х1, ..., хн)к другой \\nпоследовательности такой же длины (з1, ..., зн), схя, зя∈Рг, например, скрытый слой в типичном кодере или декодере \\nпоследовательной трансдукции. Мотивируя наше использование самовнимания, мы рассматриваем три желательных \\nусловия.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 5}, page_content='последовательной трансдукции. Мотивируя наше использование самовнимания, мы рассматриваем три желательных \\nусловия.\\nОдин из них — общая вычислительная сложность на слой. Другой — объем вычислений, которые можно \\nраспараллелить, измеряемый минимальным количеством требуемых последовательных операций.\\nТретий — длина пути между долгосрочными зависимостями в сети. Изучение долгосрочных зависимостей \\nявляется ключевой проблемой во многих задачах преобразования последовательностей. Одним из ключевых \\nфакторов, влияющих на способность изучать такие зависимости, является длина путей, которые должны пройти \\nпрямые и обратные сигналы в сети. Чем короче эти пути между любой комбинацией позиций во входных и \\nвыходных последовательностях, тем легче изучать долгосрочные зависимости [12]. Поэтому мы также \\nсравниваем максимальную длину пути между любыми двумя входными и выходными позициями в сетях, \\nсостоящих из разных типов слоев.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 5}, page_content='сравниваем максимальную длину пути между любыми двумя входными и выходными позициями в сетях, \\nсостоящих из разных типов слоев.\\nКак отмечено в Таблице 1, слой внутреннего внимания связывает все позиции с постоянным числом последовательно \\nвыполняемых операций, тогда как рекуррентный слой требует О(н)последовательные операции. С точки зрения \\nвычислительной сложности слои внутреннего внимания быстрее, чем рекуррентные слои, когда последовательность\\n6'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 6}, page_content='длина нменьше размерности представления г, что чаще всего происходит с представлениями предложений, \\nиспользуемыми современными моделями в машинном переводе, такими как представления частей слов [38] и пар \\nбайтов [31]. Для улучшения вычислительной производительности для задач, включающих очень длинные \\nпоследовательности, внутреннее внимание может быть ограничено рассмотрением только окрестности размером гв \\nпоследовательности ввода, центрированной вокруг соответствующей позиции вывода. Это увеличит максимальную \\nдлину пути доО(н/р). Мы планируем более подробно изучить этот подход в будущей работе.\\nОдин сверточный слой с шириной ядра к < нне соединяет все пары входных и выходных \\nпозиций. Для этого требуется стек О(н/к)сверточные слои в случае смежных ядер, илиО(\\nбревно к(н))в случае расширенных сверток [18], увеличивая длину самых длинных путей \\nмежду любыми двумя позициями в сети. Сверточные слои, как правило, дороже, чем'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 6}, page_content='бревно к(н))в случае расширенных сверток [18], увеличивая длину самых длинных путей \\nмежду любыми двумя позициями в сети. Сверточные слои, как правило, дороже, чем \\nрекуррентные слои, в разк. Отделимые свертки [6], однако, значительно уменьшают \\nсложность, О(к·н·г+н·г2). Даже ск=нОднако сложность разделимой свертки равна комбинации \\nслоя внутреннего внимания и слоя точечной прямой связи, подход, который мы используем \\nв нашей модели.\\nВ качестве побочного эффекта, самовнимание может дать более интерпретируемые модели. Мы проверяем распределение \\nвнимания из наших моделей и представляем и обсуждаем примеры в приложении. Мало того, что отдельные головы внимания \\nявно учатся выполнять различные задачи, многие, по-видимому, демонстрируют поведение, связанное с синтаксической и \\nсемантической структурой предложений.\\n5 Обучение\\nВ этом разделе описывается режим обучения наших моделей.\\n5.1 Обучающие данные и пакетирование'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 6}, page_content='семантической структурой предложений.\\n5 Обучение\\nВ этом разделе описывается режим обучения наших моделей.\\n5.1 Обучающие данные и пакетирование\\nМы обучались на стандартном наборе данных WMT 2014 English- German, состоящем из примерно 4,5 миллионов пар \\nпредложений. Предложения были закодированы с использованием кодирования пар байтов [3], которое имеет общий \\nисходно- целевой словарь из примерно 37000 токенов. Для английского- французского мы использовали значительно \\nбольший набор данных WMT 2014 English- French, состоящий из 36M предложений и разделенных токенов на словарь из \\n32000 словосочетаний [38]. Пары предложений были объединены вместе по приблизительной длине последовательности. \\nКаждая обучающая партия содержала набор пар предложений, содержащих примерно 25000 исходных токенов и 25000 \\nцелевых токенов.\\n5.2 Оборудование и график\\nМы обучили наши модели на одной машине с 8 графическими процессорами NVIDIA P100. Для наших базовых моделей,'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 6}, page_content='целевых токенов.\\n5.2 Оборудование и график\\nМы обучили наши модели на одной машине с 8 графическими процессорами NVIDIA P100. Для наших базовых моделей, \\nиспользующих гиперпараметры, описанные в статье, каждый шаг обучения занимал около 0,4 секунды. Мы обучили \\nбазовые модели в общей сложности 100 000 шагов или 12 часов. Для наших больших моделей (описанных в нижней \\nстроке таблицы 3) время шага составило 1,0 секунды. Большие модели обучались в течение 300 000 шагов (3,5 дня).\\n5.3 Оптимизатор\\nМы использовали оптимизатор Adam [20] сβ1=0.9,β2=0.98иϵ=10−9. Мы варьировали \\nскорость обучения в ходе обучения по формуле:\\nlrate =г−0.5модель ·мин( шаг_число −0.5, шаг_число ·разогревать _шаги−1.5) (3)\\nЭто соответствует линейному увеличению скорости обучения для первого разогревать _шаги шаги обучения, и \\nуменьшая его затем пропорционально обратному квадратному корню из номера шага. Мы использовали  \\nразогревать _шаги =4000.\\n5.4 Регуляризация\\nВ ходе обучения мы применяем три типа регуляризации:\\n7'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 7}, page_content='Таблица 2: Transformer показывает лучшие результаты BLEU, чем предыдущие современные модели, на тестах newstest2014 с \\nанглийского на немецкий и с английского на французский языки при значительно меньших затратах на обучение.\\nБЛЮ Стоимость обучения (FLOP)\\nEN-DEМодель\\nEN-DE EN-FR\\n23.75EN-FR\\nБайтНет [18]\\nDeep- Att + PosUnk [39] \\nGNMT + RL [38]\\nConvS2S [9]\\nМО [32]39.2\\n39.921.0·1020\\n1.4·1020\\n1.5·1020\\n1.2·102024.6\\n25.16 40.46\\n26.03 40.562.3·1019\\n9.6·1018\\n2.0·1019\\nАнсамбль Deep- Att + PosUnk [39] \\nАнсамбль GNMT + RL [38] \\nАнсамбль ConvS2S [9]40.4\\n26.30 41.16\\n26.36 41.298.0·1020\\n1.1·1021\\n1.2·10211.8·1020\\n7.7·1019\\nТрансформер (базовая модель) \\nТрансформер (большой)27,3 38,1\\n28,4 41,83.3·1018\\n2.3·1019\\nОстаточный отсев Мы применяем dropout [33] к выходу каждого подслоя, прежде чем он будет добавлен к \\nвходу подслоя и нормализован. Кроме того, мы применяем dropout к суммам вложений и позиционных \\nкодировок в стеках кодера и декодера. Для базовой модели мы используем скорость  Пуронить =0.1.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 7}, page_content='входу подслоя и нормализован. Кроме того, мы применяем dropout к суммам вложений и позиционных \\nкодировок в стеках кодера и декодера. Для базовой модели мы используем скорость  Пуронить =0.1.\\nСглаживание этикеток В ходе обучения мы использовали сглаживание меток стоимости ϵлс=0.1 [36] Это снижает \\nрастерянность, поскольку модель учится быть более неуверенной, но повышает точность и оценку BLEU.\\n6 результатов\\n6.1 Машинный перевод\\nВ задаче перевода с английского на немецкий язык в рамках теста WMT 2014 модель большого трансформатора \\n(Transformer (big) в таблице 2) превосходит лучшие ранее представленные модели (включая ансамбли) более чем на2.0 \\nBLEU, устанавливающий новую современную оценку BLEU 28.4.Конфигурация этой модели указана в нижней строке \\nТаблицы 3. Обучение заняло 3.5дней на8Графические процессоры P100. Даже наша базовая модель превосходит все ранее \\nопубликованные модели и ансамбли, при этом затраты на обучение составляют лишь малую часть затрат на обучение'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 7}, page_content='опубликованные модели и ансамбли, при этом затраты на обучение составляют лишь малую часть затрат на обучение \\nлюбой из конкурирующих моделей.\\nВ задании по переводу с английского на французский язык WMT 2014 наша большая модель набрала оценку BLEU\\n41.0, превзойдя все ранее опубликованные отдельные модели, менее чем за1/4стоимость обучения предыдущей \\nсовременной модели. Модель Transformer (большая), обученная для английского- французского, использовала \\nпоказатель отсева Пуронить =0.1,вместо 0.3.\\nДля базовых моделей мы использовали одну модель, полученную путем усреднения последних 5 контрольных точек, \\nкоторые были записаны с 10-минутными интервалами. Для больших моделей мы усреднили последние 20 контрольных \\nточек. Мы использовали поиск пучка с размером пучка 4и штраф за длину α=0.6 [38]. Эти гиперпараметры были выбраны \\nпосле экспериментов на наборе разработки. Мы установили максимальную длину выходных данных во время вывода на'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 7}, page_content='после экспериментов на наборе разработки. Мы установили максимальную длину выходных данных во время вывода на \\nвходную длину +50,но прекращайте раньше, если это возможно [38].\\nТаблица 2 суммирует наши результаты и сравнивает наше качество перевода и затраты на обучение с другими архитектурами \\nмоделей из литературы. Мы оцениваем количество операций с плавающей точкой, используемых для обучения модели, умножая \\nвремя обучения, количество используемых графических процессоров и оценку поддерживаемой мощности одинарной точности с \\nплавающей точкой каждого графического процессора 5.\\n6.2 Варианты моделей\\nЧтобы оценить важность различных компонентов Трансформера, мы варьировали нашу базовую модель \\nразличными способами, измеряя изменение производительности при переводе с английского на немецкий на\\n5Мы использовали значения 2,8, 3,7, 6,0 и 9,5 TFLOPS для K80, K40, M40 и P100 соответственно.\\n8'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 8}, page_content='Таблица 3: Варианты архитектуры Transformer. Неперечисленные значения идентичны значениям базовой \\nмодели. Все метрики взяты из набора для разработки перевода с английского на немецкий, newstest2013. \\nПеречисленные оплошности указаны на слово, согласно нашей кодировке пар байтов, и их не следует \\nсравнивать с оплошностями на слово.\\nтренироваться\\nшагиЗПП\\n(разраб)БЛЮ\\n(разраб)параметры\\n×106Н гмодель гфф час гк гв Пуронить ϵлс\\nбаза 6 512 2048 8 64 64 0.1 0.1 100К 4,92 25,8 65\\n1\\n4\\n16\\n32512\\n128\\n32\\n16512\\n128\\n32\\n165.29 24.9\\n5.00 25.5\\n4,91 25,8\\n5.01 25.4(А)\\n16\\n325.16 25.1\\n5.01 25.458\\n60(Б)\\n2\\n4\\n86.11 23.7\\n5.19 25.3\\n4,88 25,5\\n5,75 24,5\\n4,66 26,0\\n5.12 25.4\\n4,75 26,236\\n50\\n80\\n28\\n168\\n53\\n90(С) 256\\n102432\\n12832\\n128\\n1024\\n4096\\n0.0\\n0.25,77 24,6\\n4,95 25,5\\n4,67 25,3\\n5,47 25,7(Д)0.0\\n0.2\\n(Э)\\nбольшойпозиционное вложение вместо синусоид 4,92 25,7\\n4.33 26.4 6 1024 4096 16 0.3 300К 213\\nнабор разработки, newstest2013. Мы использовали поиск луча, как описано в предыдущем разделе, но без усреднения'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 8}, page_content='4.33 26.4 6 1024 4096 16 0.3 300К 213\\nнабор разработки, newstest2013. Мы использовали поиск луча, как описано в предыдущем разделе, но без усреднения \\nконтрольных точек. Мы представляем эти результаты в Таблице 3.\\nВ строках Таблицы 3 (A) мы варьируем количество головок внимания и размеры ключа и значения внимания, \\nсохраняя объем вычислений постоянным, как описано в Разделе 3.2.2. Хотя внимание с одной головкой на 0,9 \\nBLEU хуже, чем наилучшая настройка, качество также падает при слишком большом количестве головок.\\nВ строках таблицы 3 (B) мы видим, что уменьшение размера клавиши внимания гкухудшает качество модели. Это говорит \\nо том, что определение совместимости — непростая задача, и что более сложная функция совместимости, чем скалярное \\nпроизведение, может оказаться полезной. Далее в строках (C) и (D) мы наблюдаем, что, как и ожидалось, более крупные \\nмодели лучше, а исключение очень полезно для избежания переобучения. В строке (E) мы заменяем наше'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 8}, page_content='модели лучше, а исключение очень полезно для избежания переобучения. В строке (E) мы заменяем наше \\nсинусоидальное позиционное кодирование на изученные позиционные вложения [9] и наблюдаем почти идентичные \\nрезультаты для базовой модели.\\n6.3 Анализ английских избирательных округов\\nЧтобы оценить, может ли Transformer обобщаться на другие задачи, мы провели эксперименты по разбору \\nанглийской выборки. Эта задача представляет определенные трудности: выходные данные подвержены сильным \\nструктурным ограничениям и значительно длиннее входных данных. Более того, модели RNN sequence- to-\\nsequence не смогли достичь современных результатов в режимах с малыми данными [37].\\nМы обучили 4-слойный трансформатор сгмодель =1024 в разделе Wall Street Journal (WSJ) Penn Treebank [25], около 40 \\nтыс. обучающих предложений. Мы также обучали его в полуконтролируемой обстановке, используя более \\nкрупные высокодостоверные и BerkleyParser корпуса из примерно 17 млн   предложений [37]. Мы использовали'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 8}, page_content='крупные высокодостоверные и BerkleyParser корпуса из примерно 17 млн   предложений [37]. Мы использовали \\nсловарь из 16 тыс. токенов для настройки только WSJ и словарь из 32 тыс. токенов для полуконтролируемой \\nнастройки.\\nМы провели лишь небольшое количество экспериментов для выбора выпадающих значений, как внимания, так и остатка \\n(раздел 5.4), скорости обучения и размера пучка на наборе разработки Раздела 22, все остальные параметры остались \\nнеизменными по сравнению с базовой моделью перевода с английского на немецкий. Во время вывода мы\\n9'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 9}, page_content='Таблица 4: Transformer хорошо обобщает данные по английскому избирательным округам (результаты приведены в разделе 23 WSJ)\\nПарсер\\nВиньялс и Кайзер эль др. (2014) [37]\\nПетров и др. (2006) [29]\\nЧжу и др. (2013) [40] \\nДайер и др. (2016) [8]\\nТрансформатор (4 слоя)\\nЧжу и др. (2013) [40] Huang & \\nHarper (2009) [14] McClosky et al. \\n(2006) [26] Виньялс и Кайзер эль \\nдр. (2014) [37]\\nТрансформатор (4 слоя)\\nЛуонг и др. (2015) [23]\\nДайер и др. (2016) [8]Обучение WSJ 23 F1\\n88.3\\n90,4\\n90,4\\n91.7\\n91.3\\n91.3\\n91.3\\n92.1\\n92.1\\n92.7\\n93.0\\n93.3Только WSJ, дискриминационный \\nТолько WSJ, дискриминационный \\nТолько WSJ, дискриминационный \\nТолько WSJ, дискриминационный\\nТолько WSJ, дискриминационный\\nполуконтролируемый\\nполуконтролируемый\\nполуконтролируемый\\nполуконтролируемый\\nполуконтролируемый\\nмногозадачность\\nгенеративный\\nувеличена максимальная длина выходных данных до входной длины +300. Мы использовали размер луча 21иα=0.\\n3 как для WSJ, так и для полуконтролируемой среды.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 9}, page_content='многозадачность\\nгенеративный\\nувеличена максимальная длина выходных данных до входной длины +300. Мы использовали размер луча 21иα=0.\\n3 как для WSJ, так и для полуконтролируемой среды.\\nНаши результаты в Таблице 4 показывают, что, несмотря на отсутствие настройки под конкретную задачу, наша модель \\nработает на удивление хорошо, показывая лучшие результаты, чем все ранее представленные модели, за исключением \\nграмматики рекуррентных нейронных сетей [8].\\nВ отличие от моделей RNN «последовательность- в-последовательность» [37], Transformer превосходит Berkeley- Parser [29] \\nдаже при обучении только на обучающем наборе WSJ из 40 тыс. предложений.\\n7 Заключение\\nВ этой работе мы представили Transformer — первую модель последовательной трансдукции, полностью основанную на внимании, \\nв которой повторяющиеся слои, наиболее часто используемые в архитектурах кодера- декодера, заменены многоголовым \\nвнутренним вниманием.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 9}, page_content='в которой повторяющиеся слои, наиболее часто используемые в архитектурах кодера- декодера, заменены многоголовым \\nвнутренним вниманием.\\nДля задач перевода Transformer может обучаться значительно быстрее, чем архитектуры, основанные на \\nрекуррентных или сверточных слоях. В задачах перевода с английского на немецкий WMT 2014 и с английского \\nна французский WMT 2014 мы достигли нового уровня мастерства. В первой задаче наша лучшая модель \\nпревосходит даже все ранее представленные ансамбли.\\nМы взволнованы будущим моделей, основанных на внимании, и планируем применять их к другим задачам. Мы планируем \\nрасширить Transformer на проблемы, связанные с модальностями ввода и вывода, отличными от текста, и исследовать локальные, \\nограниченные механизмы внимания для эффективной обработки больших объемов ввода и вывода, таких как изображения, аудио \\nи видео. Сделать генерацию менее последовательной — еще одна наша исследовательская цель.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 9}, page_content='и видео. Сделать генерацию менее последовательной — еще одна наша исследовательская цель.\\nКод, который мы использовали для обучения и оценки наших моделей, доступен по адресу https://\\ngithub.com/ tensorflow/ tensor2tensor.\\nБлагодарности Мы благодарны Нал Кальхбреннер и Стефану Гоувсу за их плодотворные \\nкомментарии, исправления и вдохновение.\\nСсылки\\n[1] Джимми Лей Ба, Джейми Райан Кирос и Джеффри Э. Хинтон. Нормализация слоев. Препринт arXiv \\narXiv:1607.06450 , 2016.\\n[2] Дмитрий Богданов, Кёнхён Чо и Йошуа Бенджио. Нейронный машинный перевод путем совместного \\nобучения выравниванию и переводу. CoRR , абс/ 1409.0473, 2014.\\n[3] Денни Бритц, Анна Голди, Минь- Тханг Луонг и Куок В. Ле. Масштабное исследование архитектур \\nнейронного машинного перевода. CoRR , абс/ 1703.03906, 2017.\\n[4] Цзяньпэн Чэн, Ли Донг и Мирелла Лапата. Долговременная кратковременная память — сети для машинного \\nчтения. Препринт arXiv arXiv:1601.06733 , 2016.\\n10'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 10}, page_content='[5] Кёнхён Чо, Барт ван Мерриенбур, Чаглар Гульчехре, Фетхи Бугарес, Хольгер Швенк и Йошуа Бенджио. \\nИзучение представлений фраз с использованием кодировщика- декодера rnn для статистического \\nмашинного перевода. CoRR , абс/ 1406.1078, 2014.\\n[6] Франсуа Шолле. Xception: Глубокое обучение с разделяемыми по глубине извилинами.\\nПрепринт arXiv arXiv:1610.02357 , 2016.\\n[7] Джуньёнг Чунг, Чаглар Гюльчехре, Кёнхён Чо и Йошуа Бенджио. Эмпирическая оценка рекуррентных \\nнейронных сетей с гейтом при моделировании последовательностей. CoRR , абс/ 1412.3555, 2014.\\n[8] Крис Дайер, Адхигуна Кункоро, Мигель Баллестерос и Ноа А. Смит. Рекуррентные нейронные \\nсетевые грамматики. ВТруды NAACL , 2016.\\n[9] Йонас Геринг, Майкл Аули, Дэвид Гранжье, Денис Яратс и Ян Н. Дофин. «Сверточная \\nпоследовательность для обучения последовательностям». препринт arXiv arXiv:1705.03122v2 , 2017.\\n[10] Алекс Грейвс. Генерация последовательностей с помощью рекуррентных нейронных сетей. Препринт arXiv \\narXiv:1308.0850 , 2013.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 10}, page_content='[10] Алекс Грейвс. Генерация последовательностей с помощью рекуррентных нейронных сетей. Препринт arXiv \\narXiv:1308.0850 , 2013.\\n[11] Каймин Хэ, Сянъюй Чжан, Шаоцин Жэнь и Цзянь Сунь. Глубокое остаточное обучение для \\nраспознавания изображений. ВТруды конференции IEEE по компьютерному зрению и \\nраспознаванию образов , страницы 770– 778, 2016.\\n[12] Зепп Хохрайтер, Йошуа Бенджио, Паоло Фраскони и Юрген Шмидхубер. Градиентный поток в \\nрекуррентных сетях: сложность изучения долгосрочных зависимостей, 2001.\\n[13] Зепп Хохрайтер и Юрген Шмидхубер. Длинная кратковременная память. Нейронные вычисления , \\n9(8):1735– 1780, 1997.\\n[14] Чжунцян Хуан и Мэри Харпер. Самообучающиеся грамматики PCFG со скрытыми аннотациями \\nна разных языках. ВТруды конференции 2009 года по эмпирическим методам обработки \\nестественного языка , страницы 832– 841. ACL, август 2009 г.\\n[15] Рафал Юзефович, Ориол Виньялс, Майк Шустер, Ноам Шазир и Йонгхуэй Ву. Исследование'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 10}, page_content='естественного языка , страницы 832– 841. ACL, август 2009 г.\\n[15] Рафал Юзефович, Ориол Виньялс, Майк Шустер, Ноам Шазир и Йонгхуэй Ву. Исследование \\nпределов моделирования языка. Препринт arXiv arXiv:1602.02410 , 2016.\\n[16] Лукаш Кайзер и Сами Бенгио. Может ли активная память заменить внимание? ВДостижения в области \\nнейронных систем обработки информации (NIPS) , 2016.\\n[17] Лукаш Кайзер и Илья Суцкевер. Нейронные графические процессоры обучаются алгоритмам. ВМеждународная \\nконференция по обучению репрезентациям (ICLR) , 2016.\\n[18] Нал Кальчбреннер, Лассе Эспехольт, Карен Симонян, Аарон ван ден Оорд, Алекс Грейвс и Корай \\nКавукчуоглу. Нейронный машинный перевод в линейном времени. препринт arXiv arXiv:1610.10099v2\\n, 2017.\\n[19] Юн Ким, Карл Дентон, Луонг Хоанг и Александр М. Раш. Структурированные сети внимания. В\\nМеждународная конференция по обучению репрезентациям , 2017.\\n[20] Дидерик Кингма и Джимми Ба. Адам: Метод стохастической оптимизации. ВМЦЛР , 2015.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 10}, page_content='Международная конференция по обучению репрезентациям , 2017.\\n[20] Дидерик Кингма и Джимми Ба. Адам: Метод стохастической оптимизации. ВМЦЛР , 2015.\\n[21] Алексей Кучаев и Борис Гинзбург. Факторизационные трюки для сетей LSTM. Препринт arXiv \\narXiv:1703.10722 , 2017.\\n[22] Чжоухан Линь, Минвэй Фэн, Цицерон Ногейра душ Сантуш, Мо Ю, Бин Сян, Боуэн Чжоу и Йошуа Бенжио. \\nСтруктурированное встраивание предложений, ориентированных на самообслуживание. Препринт arXiv \\narXiv:1703.03130 , 2017.\\n[23] Минь- Тханг Луонг, Куок В. Ле, Илья Суцкевер, Ориол Виньялс и Лукаш Кайзер. Многозадачная \\nпоследовательность для последовательного обучения. Препринт arXiv arXiv:1511.06114 , 2015.\\n[24] Минь- Тханг Луонг, Хиеу Фам и Кристофер Д. Мэннинг. Эффективные подходы к нейронному \\nмашинному переводу на основе внимания. Препринт arXiv arXiv:1508.04025 , 2015.\\n11'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 11}, page_content='[25] Митчелл П. Маркус, Мэри Энн Марцинкевич и Беатрис Санторини. Создание большого аннотированного \\nкорпуса английского языка: Penn Treebank. Компьютерная лингвистика , 19(2):313– 330, 1993.\\n[26] Дэвид МакКлоски, Юджин Чарняк и Марк Джонсон. Эффективное самообучение синтаксическому анализу. В \\nТруды конференции по технологиям естественного языка NAACL, Основная конференция , страницы 152–\\n159. ACL, июнь 2006 г.\\n[27] Анкур Парих, Оскар Тэкстрем, Дипаньян Дас и Якоб Ушкорейт. Разложимая модель внимания. В\\nЭмпирические методы обработки естественного языка , 2016.\\n[28] Ромен Паулюс, Кайминг Сюн и Ричард Сохер. Глубоко укрепленная модель для абстрактного \\nрезюмирования. Препринт arXiv arXiv:1705.04304 , 2017.\\n[29] Слав Петров, Леон Барретт, Ромен Тибо и Дэн Кляйн. Изучение точной, компактной и \\nинтерпретируемой древовидной аннотации. ВТруды 21-й Международной конференции по \\nкомпьютерной лингвистике и 44-го ежегодного собрания ACL, страницы 433– 440. ACL, июль \\n2006 г.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 11}, page_content='интерпретируемой древовидной аннотации. ВТруды 21-й Международной конференции по \\nкомпьютерной лингвистике и 44-го ежегодного собрания ACL, страницы 433– 440. ACL, июль \\n2006 г.\\n[30] Офир Пресс и Лиор Вольф. Использование выходного встраивания для улучшения языковых моделей.\\nПрепринт arXiv arXiv:1608.05859 , 2016.\\n[31] Рико Сеннрих, Барри Хэддоу и Александра Бирч. Нейронный машинный перевод редких слов с \\nподсловными единицами. Препринт arXiv arXiv:1508.07909 , 2015.\\n[32] Ноам Шазир, Азалия Мирхосейни, Кшиштоф Мазиаж, Энди Дэвис, Куок Ле, Джеффри Хинтон и Джефф \\nДин. Возмутительно большие нейронные сети: слой редко- контролируемой смеси экспертов.\\nПрепринт arXiv arXiv:1701.06538 , 2017.\\n[33] Нитиш Шривастава, Джеффри Э. Хинтон, Алекс Крижевский, Илья Суцкевер и Руслан Салахутдинов. \\nDropout: простой способ предотвратить переобучение нейронных сетей. Журнал исследований \\nмашинного обучения , 15(1):1929– 1958, 2014.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 11}, page_content='Dropout: простой способ предотвратить переобучение нейронных сетей. Журнал исследований \\nмашинного обучения , 15(1):1929– 1958, 2014.\\n[34] Сайнбаяр Сухэ- Батор, Артур Шлам, Джейсон Уэстон и Роб Фергюс. Сквозные сети памяти. В \\nредакторах К. Кортеса, Н. Д. Лоуренса, Д. Д. Ли, М. Сугиямы и Р. Гарнетта:  Достижения в \\nобласти нейронных систем обработки информации 28, страницы 2440– 2448. Curran Associates, \\nInc., 2015.\\n[35] Илья Суцкевер, Ориол Виньялс и Куок В. В. Ле. Последовательность к последовательности обучения с нейронными \\nсетями. ВДостижения в области нейронных систем обработки информации , страницы 3104– 3112, 2014.\\n[36] Кристиан Сегеди, Винсент Ванхоук, Сергей Иоффе, Джонатан Шленс и Збигнев Война. \\nПереосмысление начальной архитектуры для компьютерного зрения. CoRR , абс/ 1512.00567, 2015.\\n[37] Виньялс и Кайзер, Ку, Петров, Суцкевер и Хинтон. Грамматика как иностранный язык. В Достижения в \\nобласти нейронных систем обработки информации , 2015.'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 11}, page_content='[37] Виньялс и Кайзер, Ку, Петров, Суцкевер и Хинтон. Грамматика как иностранный язык. В Достижения в \\nобласти нейронных систем обработки информации , 2015.\\n[38] Юнхуэй Ву, Майк Шустер, Чжифэн Чен, Куок В. Ле, Мохаммад Норузи, Вольфганг Машери, Максим \\nКрикун, Юань Цао, Цинь Гао, Клаус Машери и др. Нейронная система машинного перевода Google: \\nпреодоление разрыва между человеческим и машинным переводом. Препринт arXiv \\narXiv:1609.08144 , 2016.\\n[39] Цзе Чжоу, Ин Цао, Сюгуан Ван, Пэн Ли и Вэй Сюй. Глубокие рекуррентные модели с быстрыми \\nсвязями для нейронного машинного перевода. CoRR , абс/ 1606.04199, 2016.\\n[40] Мухуа Чжу, Юэ Чжан, Вэньлян Чен, Минь Чжан и Цзинбо Чжу. Быстрый и точный анализ \\nкомпонентов сдвиг- сокращение. ВТруды 51-го ежегодного собрания ACL (Том 1: Длинные \\nдоклады) , страницы 434– 443. ACL, август 2013 г.\\n12'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 12}, page_content='Визуализации внимания\\nРисунок 3: Пример механизма внимания, следующего за дальними зависимостями в самовнимании \\nкодировщика в слое 5 из 6. Многие из головок внимания обращают внимание на дальнюю зависимость глагола \\n«making», завершая фразу «making...more difficulty». Здесь показаны внимания только для слова «making». Разные \\nцвета представляют разные головы. Лучше всего просматривать в цвете.\\n13Это\\nявляется\\nв\\nэтот\\nдух\\nчто\\nа\\nбольшинство\\nиз\\nамериканский\\nправительства\\nиметь\\nпрошедший\\nновый\\nзаконы\\nс\\n2009\\nизготовление\\nthe\\nрегистрация\\nили\\nголосование\\nпроцесс\\nболее\\nтрудный\\n.\\n<ЭОС>\\n<пад>\\n<пад>\\n<пад>\\n<пад>\\n<пад>\\n<пад>Это\\nявляется\\nв\\nэтот\\nдух\\nчто\\nа\\nбольшинство\\nиз\\nамериканский\\nправительства\\nиметь\\nпрошедший\\nновый\\nзаконы\\nс\\n2009\\nизготовление\\nthe\\nрегистрация\\nили\\nголосование\\nпроцесс\\nболее\\nтрудный\\n.\\n<ЭОС>\\n<пад>\\n<пад>\\n<пад>\\n<пад>\\n<пад>\\n<пад>'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 13}, page_content='Фягу повторно4:Тваэоттнтинаон объявлениес, аlsoвлауэ-э5o 6, по-видимому, участвует в разрешении анафоры. Вверху:\\nns от просто слова «its» для привлечения внимания возглавляет 5 s \\nслово.ф, а\\nФ\\nаull\\nйв\\n6.тru\\nНтио\\nтенс\\nтадля\\nттон\\nеобъявление\\nтт5.\\nнтиБо\\nнатто\\nам:\\nеЯвляется\\nэ-эо\\nсрвыхла\\nсТед\\nарв\\nпфдесять\\nилитио\\nтхи о хэ\\n14The\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчто\\nмы\\nявляются\\nотсутствующий\\n,\\nв\\nмой\\nмнение\\n.\\n<ЭОС>\\n<пад>The\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчтоThe\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчтоThe\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчто\\nмымы мы\\nявляются\\nотсутствующий\\n,\\nв\\nмой\\nмнение\\n.\\n<ЭОС>\\n<пад>являются\\nотсутствующий\\n,\\nв\\nмой\\nмнение\\n.\\n<ЭОС>\\n<пад>являются\\nотсутствующий\\n,\\nв\\nмой\\nмнение\\n.\\n<ЭОС>\\n<пад>'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 14}, page_content='Фиг\\nнуре 5: Многие из головок внимания демонстрируют поведение, которое, по-видимому, связано со структурой\\nэй сетruсе.Втгветвосусчас\\nсехм\\nвыаplэсао\\nйбве, фромтдве разные головки от самовосприятия кодировщика \\nдля разных задач. атлавыг 5из6.Чтечасеагслеарлларекперфг\\n15The\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчто\\nмы\\nявляются\\nотсутствующий\\n,\\nв\\nмой\\nмнение\\n.\\n<ЭОС>\\n<пад>The\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчтоThe\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчтоThe\\nЗакон\\nволя\\nникогда\\nбыть\\nидеальный\\n,\\nно\\nего\\nприложение\\nдолжен\\nбыть\\nтолько\\n-\\nэтот\\nявляется\\nчто\\nмы мы мы\\nявляются\\nотсутствующий\\n,\\nвявляются\\nотсутствующий\\n,\\nвявляются\\nотсутствующий\\n,\\nв\\nмой мой мой\\nмнение\\n.\\n<ЭОС>\\n<пад>мнение\\n.\\n<ЭОС>\\n<пад>мнение\\n.\\n<ЭОС>\\n<пад>')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(separators=['\\n', '\\n\\n'], chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1729264384858,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "5LElK29Ayvtm",
    "outputId": "fab8cb05-d830-40e0-d0a6-b0d57d15e84e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1729264388717,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "OkMwNYKLzl0Q",
    "outputId": "1dbec26a-9144-49bc-dbb1-ed9a18dd5bab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='При условии указания надлежащего источника Google настоящим предоставляет разрешение на \\nвоспроизведение таблиц и рисунков в этой статье исключительно для использования в журналистских или\\nнаучные труды.\\nВнимание — это все, что вам нужно\\nАшиш Васвани ∗\\nGoogle Мозг\\navaswani@google.comНоам Шазир ∗\\nGoogle МозгНики Пармар ∗\\nИсследования GoogleЯкоб Ушкорейт ∗\\nИсследования Google\\nnoam@google.com nikip@google.com usz@google.com\\nЛлион Джонс ∗\\nИсследования Google\\nllion@google.comЭйдан Н. Гомес ∗ †\\nУниверситет Торонто\\naidan@cs.toronto.eduЛукаш Кайзер ∗\\nGoogle Мозг\\nlukaszkaiser@google.com\\nИлья Полосухин ∗ ‡\\nillia.polosukhin@gmail.com\\nАбстрактный\\nДоминирующие модели последовательной трансдукции основаны на сложных рекуррентных или \\nсверточных нейронных сетях, которые включают кодер и декодер. Наиболее эффективные модели \\nтакже соединяют кодер и декодер через механизм внимания. Мы предлагаем новую простую \\nсетевую архитектуру, Transformer, основанную исключительно на механизмах внимания, полностью'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='также соединяют кодер и декодер через механизм внимания. Мы предлагаем новую простую \\nсетевую архитектуру, Transformer, основанную исключительно на механизмах внимания, полностью \\nобходясь без рекуррентности и сверток. Эксперименты с двумя задачами машинного перевода \\nпоказывают, что эти модели превосходят по качеству, будучи более параллелизуемыми и требуя \\nзначительно меньше времени на обучение. Наша модель достигает 28,4 BLEU в задаче перевода с \\nанглийского на немецкий язык WMT 2014, улучшая существующие лучшие результаты, включая \\nансамбли, более чем на 2 BLEU. В задаче перевода с английского на французский язык WMT 2014 \\nнаша модель устанавливает новый современный показатель BLEU для одной модели в 41,8 после \\nобучения в течение 3,5 дней на восьми графических процессорах, что составляет небольшую часть \\nзатрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо \\nобобщается и на другие задачи, успешно применяя его для анализа английской электоральной'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='затрат на обучение лучших моделей из литературы. Мы показываем, что Transformer хорошо \\nобобщается и на другие задачи, успешно применяя его для анализа английской электоральной \\nаудитории как с большими, так и с ограниченными обучающими данными.\\n∗Равный вклад. Порядок перечисления случайный. Якоб предложил заменить RNN на самовнимание и начал работу по оценке \\nэтой идеи. Ашиш с Ильей спроектировали и реализовали первые модели Transformer и принимали решающее участие во всех \\nаспектах этой работы. Ноам предложил масштабированное внимание скалярного произведения, внимание с несколькими головами \\nи представление позиции без параметров и стал другим человеком, вовлеченным почти в каждую деталь. Ники спроектировал, \\nреализовал, настроил и оценил бесчисленное количество вариантов моделей в нашей исходной кодовой базе и tensor2tensor. \\nЛлион также экспериментировал с новыми вариантами моделей, отвечал за нашу исходную кодовую базу и эффективный вывод и'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 0}, page_content='Ллион также экспериментировал с новыми вариантами моделей, отвечал за нашу исходную кодовую базу и эффективный вывод и \\nвизуализацию. Лукаш и Эйдан провели бесчисленное количество долгих дней, проектируя различные части и реализуя \\ntensor2tensor, заменив нашу более раннюю кодовую базу, значительно улучшив результаты и значительно ускорив наши \\nисследования.\\n†Работа, выполненная во время работы в Google Brain.\\n‡Работа выполнялась во время работы в Google Research.\\n31-я конференция по нейронным системам обработки информации (NIPS 2017), Лонг- Бич, Калифорния, США.arXiv:1706.03762v7 [cs.CL] 2 августа 2023 г.\\nПеревод: английский - русский - www.onlinedoctranslator.com'),\n",
       " Document(metadata={'source': 'attention_ru.pdf', 'page': 1}, page_content='1 Введение\\nРекуррентные нейронные сети, в частности, долговременная кратковременная память [13] и управляемые \\nрекуррентные [7] нейронные сети, прочно утвердились как современные подходы в моделировании \\nпоследовательностей и проблемах трансдукции, таких как моделирование языка и машинный перевод [35, 2, 5]. \\nС тех пор многочисленные усилия продолжают расширять границы рекуррентных языковых моделей и \\nархитектур кодера- декодера [38, 24, 15].\\nРекуррентные модели обычно факторизуют вычисления по позициям символов входных и выходных \\nпоследовательностей. Выравнивая позиции по шагам во времени вычислений, они генерируют последовательность \\nскрытых состояний час т, как функция предыдущего скрытого состояния час т−1и вход для позиции т. Эта изначально \\nпоследовательная природа исключает параллелизацию в обучающих примерах, что становится критическим при более \\nдлинных последовательностях, поскольку ограничения памяти ограничивают пакетирование между примерами.')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY2UIYw7VMyc"
   },
   "source": [
    "### Попытка заменить OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8189,
     "status": "ok",
     "timestamp": 1729266444848,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "Az3X9r49ELr5",
    "outputId": "9dbc51b3-1760-4d52-ac29-b2c989fd9dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/255.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/255.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# !pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22515,
     "status": "ok",
     "timestamp": 1729268812172,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "beS6zvJWcalV",
    "outputId": "ca426cf6-712c-4130-ad3e-075d2f2a9932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m67.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4359,
     "status": "ok",
     "timestamp": 1729270067311,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "GOMbt4OogdY-",
    "outputId": "9702e8e4-d15b-48d9-a95f-d0c6241a17f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1729269520174,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "epl-qWqMVz-R"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1729268589648,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "5ZFgy7I8T6lr",
    "outputId": "6c278419-d607-48a7-a2f9-04e541a9151b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#model_kwargs = {'device': 'cuda'}\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "ok",
     "timestamp": 1729269366847,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "2eMqLWoVcT-E"
   },
   "outputs": [],
   "source": [
    "query = 'функция внимания'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "executionInfo": {
     "elapsed": 9083,
     "status": "ok",
     "timestamp": 1729269282588,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "RgdAcpTmWsy4"
   },
   "outputs": [],
   "source": [
    "db_chroma = Chroma.from_documents(documents, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1729269436589,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "Kltl0BoqeHpx",
    "outputId": "37490b8d-ebab-4a82-88a4-5b5e3fc7b033"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'матрицу В. Ключи и значения также упакованы вместе в матрицы. КиВ.Мы вычисляем матрицу выходов \\nследующим образом:\\nККТВнимание( К, К, В) =софтмакс( √ )В (1)гк\\nДве наиболее часто используемые функции внимания — это аддитивное внимание [2] и внимание скалярного произведения \\n(мультипликативное). Внимание скалярного произведения идентично нашему алгоритму, за исключением масштабирующего \\nфактора √1.Аддитивное внимание вычисляет функцию совместимости с использованием сети прямой связи сгк\\nодин скрытый слой. Хотя эти два метода схожи по теоретической сложности, метод внимания к скалярному произведению на практике гораздо \\nбыстрее и эффективнее с точки зрения занимаемой памяти, поскольку его можно реализовать с использованием высокооптимизированного \\nкода умножения матриц.\\nВ то время как для малых значений гкоба механизма работают одинаково, аддитивное внимание превосходит внимание,'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retireved_results = db_chroma.similarity_search(query)\n",
    "#retireved_results\n",
    "retireved_results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "executionInfo": {
     "elapsed": 7160,
     "status": "ok",
     "timestamp": 1729270090383,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "3xuVjs-rfY4R"
   },
   "outputs": [],
   "source": [
    "db_faiss = FAISS.from_documents(documents, embedding=hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1729270133172,
     "user": {
      "displayName": "Andrey Petrashko",
      "userId": "02774772516973286917"
     },
     "user_tz": -420
    },
    "id": "AHfbsaKdgAin",
    "outputId": "cbd382d9-c923-45a4-c61d-128b9c8f1a37"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'матрицу В. Ключи и значения также упакованы вместе в матрицы. КиВ.Мы вычисляем матрицу выходов \\nследующим образом:\\nККТВнимание( К, К, В) =софтмакс( √ )В (1)гк\\nДве наиболее часто используемые функции внимания — это аддитивное внимание [2] и внимание скалярного произведения \\n(мультипликативное). Внимание скалярного произведения идентично нашему алгоритму, за исключением масштабирующего \\nфактора √1.Аддитивное внимание вычисляет функцию совместимости с использованием сети прямой связи сгк\\nодин скрытый слой. Хотя эти два метода схожи по теоретической сложности, метод внимания к скалярному произведению на практике гораздо \\nбыстрее и эффективнее с точки зрения занимаемой памяти, поскольку его можно реализовать с использованием высокооптимизированного \\nкода умножения матриц.\\nВ то время как для малых значений гкоба механизма работают одинаково, аддитивное внимание превосходит внимание,'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retireved_results = db_faiss.similarity_search(query)\n",
    "#retireved_results\n",
    "retireved_results[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jitlHcNiC7G"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMHncclXdEDb/b70vadcKDV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
