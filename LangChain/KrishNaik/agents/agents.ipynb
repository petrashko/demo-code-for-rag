{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q arxiv\n",
    "#%pip install -q arxiv\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "#\n",
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "from langchain_community.tools import ArxivQueryRun\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "#from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "#\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заменить OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Progi\\PythonProg\\NLP\\LangChain\\KrishNaik\\.venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "#model_kwargs = {'device': 'cuda'}\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_id,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000268071DBA10>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loader = WebBaseLoader('https://docs.smith.langchain.com/')\n",
    "loader = WebBaseLoader('https://huggingface.co/learn/nlp-course/ru/chapter1/2')\n",
    "docs = loader.load()\n",
    "\n",
    "# Загруженную HTML-страницу сохраняем в векторной базе данных. И создаем retriever\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "#vectordb = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "vectordb = FAISS.from_documents(documents, hf_embeddings)\n",
    "retriever = vectordb.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface_nlp_course'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever, 'huggingface_nlp_course',\n",
    "    'Поиск информации о huggingface. Для всех вопросов о huggingface по  NLP, ты должен использовать этот инструмент!'\n",
    ")\n",
    "retriever_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wikipedia'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wiki Tool\n",
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "\n",
    "wiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "wiki.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Аналог wiki для научных статей\n",
    "arxiv_wrapper = ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=200)\n",
    "\n",
    "arxiv = ArxivQueryRun(api_wrapper=arxiv_wrapper)\n",
    "arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'e:\\\\Progi\\\\PythonProg\\\\NLP\\\\LangChain\\\\KrishNaik\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200, arxiv_result=<class 'arxiv.Result'>)),\n",
       " Tool(name='huggingface_nlp_course', description='Поиск информации о huggingface. Для всех вопросов о huggingface по  NLP, ты должен использовать этот инструмент!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000268472C9300>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000268071DBA10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000268472C9760>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000268071DBA10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'))]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Порядок поиска информации для агентов:\n",
    "# сначала в wiki, затем в arxiv, затем в retriever_tool\n",
    "tools = [wiki, arxiv, retriever_tool]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='sutyrin/saiga_mistral_7b')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llm = Ollama(model='sutyrin/saiga_mistral_7b')\n",
    "llm = ChatOllama(model='sutyrin/saiga_mistral_7b')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Progi\\PythonProg\\NLP\\LangChain\\KrishNaik\\.venv\\Lib\\site-packages\\langchain\\hub.py:86: DeprecationWarning: The `langchainhub sdk` is deprecated.\n",
      "Please use the `langsmith sdk` instead:\n",
      "  pip install langsmith\n",
      "Use the `pull_prompt` method.\n",
      "  res_dict = client.pull_repo(owner_repo_commit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull('hwchase17/openai-functions-agent')\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentExecutor(verbose=True, agent=RunnableMultiActionAgent(runnable=RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatOllama(model='sutyrin/saiga_mistral_7b'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'huggingface_nlp_course', 'description': 'Поиск информации о huggingface. Для всех вопросов о huggingface по  NLP, ты должен использовать этот инструмент!', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query']}}}, {'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}}}]})\n",
       "| OpenAIToolsAgentOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='huggingface_nlp_course', description='Поиск информации о huggingface. Для всех вопросов о huggingface по  NLP, ты должен использовать этот инструмент!', args_schema=<class 'langchain_core.tools.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x00000268472C9300>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000268071DBA10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n'), coroutine=functools.partial(<function _aget_relevant_documents at 0x00000268472C9760>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000268071DBA10>), document_prompt=PromptTemplate(input_variables=['page_content'], template='{page_content}'), document_separator='\\n\\n')), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'e:\\\\Progi\\\\PythonProg\\\\NLP\\\\LangChain\\\\KrishNaik\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=200)), ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=200, arxiv_result=<class 'arxiv.Result'>))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Агенты отвечают за последовательность действий для генерации ответа\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "agent_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mВ Hugging Face есть множество моделей, которые могут быть использованы для решения различных задач. Вот некоторые из них:\n",
      "\n",
      "1. **Классификация текста**: Модель классифицирует текст на основе его категории (например, спам или неспам).\n",
      "2. **Разметка текста**: Модель выделяет определенные сущности в тексте (например, имена людей, даты и т.д.).\n",
      "3. **Получение ответов на вопросы**: Модель генерирует ответ на заданный вопрос.\n",
      "4. **Перевод текста**: Модель переводит текст с одного языка на другой.\n",
      "5. **Синтез текста**: Модель создает новый текст, основанный на данных, предоставленных пользователем.\n",
      "6. **Получение рекомендаций**: Модель предлагает пользователю соответствующие ему товары или контент.\n",
      "7. **Определение эмоций**: Модель определяет эмоцию, которую выражает текст (например, радость, грусть и т.д.).\n",
      "8. **Разметка сентиментов**: Модель определяет положительный или негативный тон текста.\n",
      "9. **Получение релевантных документов**: Модель выдает документы, которые наиболее соответствуют заданному запросу.\n",
      "10. **Разметка времени**: Модель определяет время, упоминаемое в тексте (например, дата и время).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Список типичных NLP-задач в huggingface',\n",
       " 'output': 'В Hugging Face есть множество моделей, которые могут быть использованы для решения различных задач. Вот некоторые из них:\\n\\n1. **Классификация текста**: Модель классифицирует текст на основе его категории (например, спам или неспам).\\n2. **Разметка текста**: Модель выделяет определенные сущности в тексте (например, имена людей, даты и т.д.).\\n3. **Получение ответов на вопросы**: Модель генерирует ответ на заданный вопрос.\\n4. **Перевод текста**: Модель переводит текст с одного языка на другой.\\n5. **Синтез текста**: Модель создает новый текст, основанный на данных, предоставленных пользователем.\\n6. **Получение рекомендаций**: Модель предлагает пользователю соответствующие ему товары или контент.\\n7. **Определение эмоций**: Модель определяет эмоцию, которую выражает текст (например, радость, грусть и т.д.).\\n8. **Разметка сентиментов**: Модель определяет положительный или негативный тон текста.\\n9. **Получение релевантных документов**: Модель выдает документы, которые наиболее соответствуют заданному запросу.\\n10. **Разметка времени**: Модель определяет время, упоминаемое в тексте (например, дата и время).'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({'input': 'Список типичных NLP-задач в huggingface'})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В Hugging Face есть несколько задач, которые могут быть использованы для обучения и тестирования моделей на основе предобученных моделей. Вот некоторые из них:\\n\\n1. **Classification**: Задача классификации предполагает, что модель должна определить категорию или класс для данного текста. Например, можно обучить модель на тексте с рецензиями и затем использовать ее для определения положительных или негативных отзывов.\\n2. **Sequence Classification**: Задача секвенциальной классификации похожа на обычную классификацию, но вместо того чтобы работать с отдельными текстами, модель должна анализировать последовательность слов или токенов. Например, можно обучить модель на серии слов и затем использовать ее для определения эмоций в тексте.\\n3. **Text Generation**: Задача генерации текста предполагает, что модель должна создавать новый текст на основе данных, которые она получила. Например, можно обучить модель на серии слов и затем использовать ее для создания нового текста в стиле оригинальной серии.\\n4. **Token Classification**: Задача токенизации предполагает, что модель должна анализировать каждый токен отдельно и выявлять его класс или категорию. Например, можно обучить модель на тексте с рецензиями и затем использовать ее для определения эмоций в каждом токене.\\n5. **Question Answering**: Задача ответа на вопрос предполагает, что модель должна анализировать текст и выявлять конкретный ответ на заданный вопрос. Например, можно обучить модель на серии слов и затем использовать ее для ответа на вопрос \"Какое животное?\"\\n6. **Sentiment Analysis**: Задача анализа эмоций предполагает, что модель должна определять эмоции или настроение в тексте. Например, можно обучить модель на серии слов и затем использовать ее для определения положительных или негативных отзывов.\\n7. **Summarization**: Задача сжатия текста предполагает, что модель должна создавать короткий обзор длинного текста. Например, можно обучить модель на серии слов и затем использовать ее для создания краткого обзора статьи.\\n8. **Translation**: Задача перевода предполагает, что модель должна переводить текст с одного языка на другой. Например, можно обучить модель на серии слов и затем использовать ее для перевода текста с английского на русский язык.\\n9. **Relation Extraction**: Задача извлечения отношений предполагает, что модель должна определять отношения между различными сущностями в тексте. Например, можно обучить модель на серии слов и затем использовать ее для определения того, кто является автором статьи.\\n10. **Coreference Resolution**: Задача резолюции косвенных ссылок предполагает, что модель должна определять, какой субъект или объект в тексте относится к другому субъекту или объекту. Например, можно обучить модель на серии слов и затем использовать ее для определения того, кто является автором статьи.\\n11. **Named Entity Recognition**: Задача распознавания сущностей предполагает, что модель должна определять различные сущности в тексте, такие как имена людей, организаций, мест и т.д. Например, можно обучить модель на серии слов и затем использовать ее для распознавания имён авторов статей.\\n12. **Dependency Parsing**: Задача анализа синтаксиса предполагает, что модель должна определять структуру предложения и отношения между его частями. Например, можно обучить модель на серии слов и затем использовать ее для определения того, кто является автором статьи.\\n13. **Semantic Role Labeling**: Задача анализа семантических ролей предполагает, что модель должна определять роли сущностей в предложении. Например, можно обучить модель на серии слов и затем использовать ее для определения того, кто является автором статьи.\\n14. **Text-to-SQL**: Задача преобразования текста в SQL предполагает, что модель должна превратить текст вопроса в соответствующий запрос на базу данных. Например, можно обучить модель на серии слов и затем использовать ее для создания запроса на базу данных по вопросу \"Какое животное?\"\\n15. **Text-to-Code**: Задача преобразования текста в код предполагает, что модель должна превратить текст вопроса в соответствующий код программы. Например, можно обучить модель на серии слов и затем использовать ее для создания кода по вопросу \"Какое животное?\"'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mВот несколько причин, почему NLP в Hugging Face может быть сложным:\n",
      "\n",
      "1. **Огромное количество моделей**: Hugging Face предоставляет более чем 200 различных моделей NLP, что может сделать выбор модели для конкретной задачи сложным.\n",
      "\n",
      "2. **Необходимость понимания архитектуры и параметров моделей**: Для правильного использования модели NLP необходимо понимание, как она работает и какие параметры можно изменить для достижения лучших результатов.\n",
      "\n",
      "3. **Оптимизация гиперпараметров**: Оптимальные значения гиперпараметров могут варьироваться в зависимости от конкретной задачи и данных, что может потребовать некоторых экспериментов для их настройки.\n",
      "\n",
      "4. **Оптимизация вычислительных ресурсов**: Некоторые модели NLP требуют большого количества вычислительных ресурсов, что может быть дорогостоящим и затратным в энергетическом отношении.\n",
      "\n",
      "5. **Оптимизация памяти**: Некоторые модели NLP требуют большого объема памяти для работы, что может быть проблемой на некоторых устройствах.\n",
      "\n",
      "6. **Необходимость понимания языка программирования**: Для использования моделей NLP в Hugging Face требуется знание языка программирования, такого как Python.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Почему NLP в huggingface это сложно?',\n",
       " 'output': 'Вот несколько причин, почему NLP в Hugging Face может быть сложным:\\n\\n1. **Огромное количество моделей**: Hugging Face предоставляет более чем 200 различных моделей NLP, что может сделать выбор модели для конкретной задачи сложным.\\n\\n2. **Необходимость понимания архитектуры и параметров моделей**: Для правильного использования модели NLP необходимо понимание, как она работает и какие параметры можно изменить для достижения лучших результатов.\\n\\n3. **Оптимизация гиперпараметров**: Оптимальные значения гиперпараметров могут варьироваться в зависимости от конкретной задачи и данных, что может потребовать некоторых экспериментов для их настройки.\\n\\n4. **Оптимизация вычислительных ресурсов**: Некоторые модели NLP требуют большого количества вычислительных ресурсов, что может быть дорогостоящим и затратным в энергетическом отношении.\\n\\n5. **Оптимизация памяти**: Некоторые модели NLP требуют большого объема памяти для работы, что может быть проблемой на некоторых устройствах.\\n\\n6. **Необходимость понимания языка программирования**: Для использования моделей NLP в Hugging Face требуется знание языка программирования, такого как Python.'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = agent_executor.invoke({'input': 'Почему NLP в huggingface это сложно?'})\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
